#!/usr/bin/env python
"""
mbank_train_flow
----------------

Executable to train a normalizing flow to reproduce the volume element of the probability distribution:

.. math
				
	p(\\theta) \propto \sqrt{|M(\\theta)|}

The script will create a dataset with random points drawn within the space an will train the flow with it.
The first part can be omitted by providing a home-made dataset.

To train a flow:

	mbank_train_flow --options-you-like

You can also load (some) options from an ini-file:

	mbank_train_flow --some-options other_options.ini

Make sure that the mbank is properly installed.

To know which options are available:

	mbank_train_flow --help
"""
import numpy as np
import matplotlib.pyplot as plt
import sys
import warnings
from ligo.lw.utils import load_filename

import torch
from torch import optim

from mbank import variable_handler, cbc_metric
from mbank.utils import updates_args_from_ini, load_PSD, avg_dist, plot_tiles_templates, get_boundaries_from_ranges
from mbank.flow import STD_GW_Flow, GW_Flow
from mbank.flow.utils import early_stopper

import emcee
from tqdm import tqdm
import argparse
import os
from lal import MTSUN_SI

############################################################################Ã 
def is_inside(theta, boundaries, variable_format):
	theta = np.atleast_2d(theta)
	if boundaries is not None:
		ids_inside = np.logical_and(np.all(theta > boundaries[0,:], axis =1), np.all(theta < boundaries[1,:], axis = 1)) #(N,)
	else:
		ids_inside = np.full((theta.shape[0],), True)
	
	m1, m2 = np.full(theta[:,0].shape, 0.), np.full(theta[:,0].shape, 0.)
	m1[ids_inside], m2[ids_inside] = var_handler.get_BBH_components(theta[ids_inside], variable_format)[:,:2].T

	if args.m1_range:
		ids_inside = np.logical_and(ids_inside, np.logical_and(m1>args.m1_range[0], m1<args.m1_range[1]))
	if args.m2_range:
		ids_inside = np.logical_and(ids_inside, np.logical_and(m2>args.m2_range[0], m2<args.m2_range[1]))
	if args.mtot_range:
		M = m1 + m2
		ids_inside = np.logical_and(ids_inside, np.logical_and(M>args.mtot_range[0], M<args.mtot_range[1]))
	if args.q_range:
		q = m1/m2
		ids_inside = np.logical_and(ids_inside, np.logical_and(q>args.q_range[0], q<args.q_range[1]))
	if args.mc_range:
		mc = (m1*m2)**(3/5)/(m1+m2)**(1/5)
		ids_inside = np.logical_and(ids_inside, np.logical_and(mc>args.mc_range[0], mc<args.mc_range[1]))
	if args.eta_range:
		eta = (m1*m2)/np.square(m1+m2)
		ids_inside = np.logical_and(ids_inside, np.logical_and(eta>args.eta_range[0], eta<args.eta_range[1]))
	return ids_inside

def initial_dataset_log_prob(mceta):
	squeeze = (np.asarray(mceta).ndim == 1)
	mceta = np.atleast_2d(mceta)
	res = np.full(mceta[:,0].shape, -100000)
	ids_inside = is_inside(mceta, np.array([[-np.inf, -np.inf],[np.inf, 0.25]]), sample_format)
	if np.any(ids_inside):
		res[ids_inside] = -(np.log(2048/(5*np.pi)) + 13/3 * np.log(np.pi*args.f_min) + 10/3 * np.log(mceta[ids_inside,0]) + 8/5 * np.log(mceta[ids_inside,1]))
	if squeeze: res = res[0]
	return res
	

def get_random_masses(boundaries):

	assert boundaries.shape == (2,2)

	n_walkers = 100
	start_points = []
	out_variable_format = '{}_nonspinning'.format(var_handler.format_info[args.variable_format]['mass_format'])

	while len(start_points)<n_walkers:
		new_points = var_handler.convert_theta(np.random.uniform(*boundaries, (100, 2)), out_variable_format, sample_format)
		new_points = new_points[is_inside(new_points, None, sample_format)]
		if len(new_points)>0:
			start_points.extend(new_points)
	start_points = np.array(start_points)[:n_walkers]
	
	sampler = emcee.EnsembleSampler(n_walkers, 2, initial_dataset_log_prob)
		#Burn-in
	points = sampler.run_mcmc(start_points, 400, tune=True, store=False, progress=args.verbose, progress_kwargs={'desc': 'Burn-in for mass samples'})
	
	new_masses = []
	for i in tqdm(range(args.n_datapoints//n_walkers+1), desc = 'Generating mass samples', disable = not args.verbose):
		points = sampler.run_mcmc(points, 20, tune=False, store=False, progress=False)
		new_masses.append(points.coords)

	new_masses = np.concatenate(new_masses, axis = 0)
	new_masses = var_handler.convert_theta(new_masses[:args.n_datapoints], sample_format, out_variable_format)

	return new_masses

##### Creating parser
parser = argparse.ArgumentParser(__doc__)
parser.add_argument(
	"--variable-format", required = False,
	help="Choose which variables to include in the bank. Valid formats are those of `mbank.handlers.variable_format`")
parser.add_argument(
	"--psd",  required = False,
	help="The input file for the PSD: it can be either a txt either a ligolw xml file")
parser.add_argument(
	"--asd",  default = False, action='store_true',
	help="Whether the input file has an ASD (sqrt of the PSD)")
parser.add_argument(
	"--ifo", default = 'L1', type=str, choices = ['L1', 'H1', 'V1'],
	help="Interferometer name: it can be L1, H1, V1. This is a field for the xml files for the PSD and the bank")
parser.add_argument(
	"--f-min",  default = 10., type=float,
	help="Minium frequency for the scalar product")
parser.add_argument(
	"--f-max",  default = 1024., type=float,
	help="Maximum frequency for the scalar product")
parser.add_argument(
	"--df",  default = None, type=float,
	help="Spacing of the frequency grid where the PSD is evaluated")
parser.add_argument(
	"--approximant", default = 'IMRPhenomPv2',
	help="LAL approximant for the bank generation")

parser.add_argument(
	"--plot", action='store_true',
	help="Whether to make some plots. They will be store in run-dir")
parser.add_argument(
	"--show", action='store_true',
	help="Whether to show the plots.")
parser.add_argument(
	"--run-dir", default = None,
	help="Output directory in which the bank will be saved. If default is used, the run name will be appended.")
parser.add_argument(
	"--run-name", default = 'flow_training',
	help="Name for the bank and tiling output file")

parser.add_argument(
	"--metric-type", default = 'symphony', type = str, choices = ['hessian', 'parabolic_fit_hessian', 'symphony'],
	help="Method to use to compute the metric.")

parser.add_argument(
	"--loss-function", default = 'll_mse', type = str, choices = STD_GW_Flow(1, 1).available_losses,
	help="Method to use to compute the metric.")
parser.add_argument(
	"--n-layers", default = 2, type = int,
	help="Number of layers for the flow model to train (applicable only if --train-flow)")
parser.add_argument(
	"--hidden-features", default = 4, type = int,
	help="Number of hidden features for the masked autoregressive flow to train. (applicable only if --train-flow)")
parser.add_argument(
	"--n-epochs", default = 10000, type = int,
	help="Number of training epochs for the flow (applicable only if --train-flow)")
parser.add_argument(
	"--learning-rate", default = 1e-3, type = float,
	help="Learning rate for the flow")
parser.add_argument(
	"--train-fraction", default = 0.85, type = float,
	help="Fraction of the dataset to use for training")
parser.add_argument(
	"--batch-size", default = 10000, type = int,
	help="Batch size for the training")
parser.add_argument(
	"--min-delta", default = 0., type = float,
	help="Parameter for early stopping. If the validation loss doesn't decrease of more than min-delta for --patience times, the training will stop")
parser.add_argument(
	"--patience", default = 5, type = int,
	help="Patience for early stopping. If the validation loss does not decrease enough for patience times, the training will stop")

parser.add_argument(
	"--n-datapoints", default = None, type = int,
	help="Number of datapoints to be stored or loaded in the dataset")
parser.add_argument(
	"--dataset", default = ['dataset.dat'], type = str, nargs = '+',
	help="Files with the datasets (more than one are accpted). If the file does not exist, the dataset will be created.")
parser.add_argument(
	"--datapoints-file", default = None, type = str,
	help="Csv with a list of points to build the dataset: for each of them the likelihood will be computed and stored in the dataset")
parser.add_argument(
	"--flow-file", default = 'flow.zip', type = str,
	help="File where the normalizing flow is saved (in zip format).")
parser.add_argument(
	"--load-flow", default = False, action = 'store_true',
	help="Whether to load the flow from file (useful for plotting purposes)")	

	#ranges for physical parameters
parser.add_argument(
	"--m1-range", default = None, type=float, nargs = 2,
	help="Range values for the mass 1 (in solar masses)")
parser.add_argument(
	"--m2-range", default = None, type=float, nargs = 2,
	help="Range values for the mass 2 (in solar masses)")
parser.add_argument(
	"--mtot-range", default = None, type=float, nargs = 2,
	help="Range values for the total masses (in solar masses).")
parser.add_argument(
	"--q-range", default = None, type=float, nargs = 2,
	help="Range values for the mass ratio.")
parser.add_argument(
	"--mc-range", default = None, type=float, nargs = 2,
	help="Range values for the total masses (in solar masses).")
parser.add_argument(
	"--eta-range", default = None, type=float, nargs = 2,
	help="Range values for the mass ratio.")
parser.add_argument(
	"--s1-range", default = [-0.99,0.99], type=float, nargs = 2,
	help="Range values for magnitude of spin 1 (if applicable)")
parser.add_argument(
	"--s2-range", default = [-0.99,0.99], type=float, nargs = 2,
	help="Range values for magnitude of spin 1 (if applicable)")
parser.add_argument(
	"--chi-range", default = [-0.99,0.99], type=float, nargs = 2,
	help="Range values for effective spin parameter (if applicable)")
parser.add_argument(
	"--theta-range", default = [-np.pi, np.pi], type=float, nargs = 2,
	help="Range values for theta angles of spins (if applicable)")
parser.add_argument(
	"--phi-range", default = [-np.pi/2, np.pi/2], type=float, nargs = 2,
	help="Range values for phi angles of spins (if applicable)")
parser.add_argument(
	"--e-range", default = [0., 0.5], type=float, nargs = 2,
	help="Range values for the eccentricity (if applicable)")
parser.add_argument(
	"--meanano-range", default = [0., 1], type=float, nargs = 2,
	help="Range values for the mean anomaly (if applicable). TODO: find a nice default...")
parser.add_argument(
	"--iota-range", default = [0., np.pi], type=float, nargs = 2,
	help="Range values for iota (if applicable)")
parser.add_argument(
	"--ref-phase-range", default = [-np.pi, np.pi], type=float, nargs = 2,
	help="Range values for reference phase (if applicable)")

parser.add_argument(
	"--ignore-boundaries", default = False, action = 'store_true',
	help="Whether to ignore the ranges given by command line. If set, the boudaries will be extracted from the dataset")
parser.add_argument(
	"--verbose", default = False, action = 'store_true',
	help="Whether to be verbose")

args, filenames = parser.parse_known_args()

	#updating from the ini file(s), if it's the case
for f in filenames:
	args = updates_args_from_ini(f, args, parser)

####################################################################################################
	######
	#	Interpreting the parser and initializing variables
	######

if (args.psd is None) or (args.variable_format is None):
	raise ValueError("The arguments psd and variable_format must be set!")

var_handler = variable_handler()
assert args.variable_format in var_handler.valid_formats, "Wrong value {} for variable-format".format(args.variable_format)
D = var_handler.D(args.variable_format)

if args.run_dir is None: args.run_dir = './out_{}/'.format(args.run_name)
if not args.run_dir.endswith('/'): args.run_dir = args.run_dir+'/'
if not os.path.exists(args.run_dir): os.makedirs(args.run_dir)
if args.psd.find('/') <0: args.psd = args.run_dir+args.psd
if args.flow_file.find('/') <0: args.flow_file = args.run_dir+args.flow_file

for i, f in enumerate(args.dataset):
	if f.find('/') <0: f = args.run_dir+f
	args.dataset[i] = f

generate_dataset = not np.any([os.path.isfile(f) for f in args.dataset])

boundaries = None
if not args.ignore_boundaries:
	format_info = var_handler.format_info[args.variable_format]
	if format_info['mass_format'] == 'm1m2':
		assert args.m1_range and args.m2_range, "If mass format is m1m2, --m1-range and --m2-range must be given"
		var1_min, var1_max = args.m1_range
		var2_min, var2_max = args.m2_range
	elif format_info['mass_format'] in ['Mq', 'logMq']:
		assert args.mtot_range and args.q_range, "If mass format is Mq, --mtot-range and --q-range must be given"
		var1_min, var1_max = args.mtot_range
		var2_min, var2_max = args.q_range
	elif format_info['mass_format'] == 'mceta':
		assert args.mc_range and args.eta_range, "If mass format is mceta, --mc-range and --eta-range must be given"
		var1_min, var1_max = args.mc_range
		var2_min, var2_max = args.eta_range
	sample_format = 'mceta_nonspinning'
	print('sample_format', sample_format)

		#Ranges for quantities other than masses
	s1_min, s1_max = args.s1_range
	s2_min, s2_max = args.s2_range
	chi_min, chi_max = args.chi_range
	theta_min, theta_max = args.theta_range
	e_min, e_max = args.e_range
	meanano_min, meanano_max = args.meanano_range
	phi_min, phi_max = args.phi_range
	iota_min, iota_max = args.iota_range
	ref_phase_min, ref_phase_max = args.ref_phase_range

	plot_folder = args.run_dir if args.plot else None

	if args.verbose: print("## Running: ", args.run_name)

	boundaries = get_boundaries_from_ranges(args.variable_format,
				(var1_min, var1_max), (var2_min, var2_max),
				chi_range = (chi_min, chi_max), s1_range = (s1_min, s1_max), s2_range = (s2_min, s2_max),
				theta_range = (theta_min, theta_max), phi_range = (phi_min, phi_max),
				iota_range = (iota_min, iota_max), ref_phase_range = (ref_phase_min, ref_phase_max),
				e_range = (e_min, e_max), meanano_range = (meanano_min, meanano_max))

if generate_dataset:

	assert args.n_datapoints, "--n-datapoints must be set if the dataset is generated"

	######
	#	Loading PSD and initializing metric
	######
	m = cbc_metric(args.variable_format,
		PSD = load_PSD(args.psd, args.asd, args.ifo, df = args.df),
		approx = args.approximant,
		f_min = args.f_min, f_max = args.f_max)

	######
	#	Drawing initial points (either from file or from sampling)
	#	Mass boundaries are complicated, while the rest is just a rectangle in the coordinates
	######

	if args.datapoints_file:
	
		datapoints = np.loadtxt(args.datapoints_file)[:args.n_datapoints]

	else:
	
		#Random sampling
			#setting spin & angles boundaries
		assert boundaries is not None, "If dataset has to be generated, option --ignore-boundaries must be removed"
		datapoints = np.random.uniform(*boundaries, (args.n_datapoints, D))
		
		new_masses = get_random_masses(boundaries[:,:2])
		datapoints[:,:2] = new_masses

	metric_list = []
	for d in tqdm(datapoints, disable = not args.verbose, desc = 'Generating dataset'):
		try:
			metric = m.get_metric(d, metric_type = args.metric_type)
			metric_list.append(np.array([*metric.flatten(), 0.5*np.log(np.abs(np.linalg.det(metric)))]))
		except KeyboardInterrupt:
			warnings.warn("KeyboardInterrupt: interuppting dataset generation and continuing with the flow training")
			break
	
	to_save = np.concatenate([datapoints[:len(metric_list)], metric_list], axis = 1)
	np.savetxt(args.dataset[0], to_save,
		header = 'variable format: {0}\ntheta ({1},) | metric ({1}*{1},) | log_pdf (1,) '.format(args.variable_format, D))

	if args.verbose: print("Dataset saved")
	del datapoints, to_save

	######
	#	Loading the dataset(s) and performing train and test splitting
	######

dataset = []
for f in args.dataset:
	if not os.path.isfile(f): continue
	
	if args.verbose: print("Loading dataset {}".format(f))
	
	new_dataset = np.loadtxt(f)
	assert new_dataset.ndim == 2, "Wrong dimensionality of the dataset, must be 2 dimensional"
	assert new_dataset.shape[1] == D**2+D+1, "Wrong format of the datset! Each row must be {} dimensional, i.e. with format D | D**2 | 1".format(D*(D+1)+1)
	
		#removing possible nans
	new_dataset = new_dataset[np.where(~np.any(np.isnan(new_dataset), axis = 1))]
	
	dataset.append(new_dataset)

dataset = np.concatenate(dataset, axis = 0)

if not args.ignore_boundaries:
	ids_ = is_inside(dataset[:,:D], boundaries, args.variable_format)
	dataset = dataset[ids_]

if not True:
	#dataset = dataset[np.where(dataset[:,-1]>-12)]
	dataset = dataset[np.where(dataset[:,-1]<=-12)]
	np.random.shuffle(dataset)
	np.savetxt('out_O3_bank_flow/dataset_paper_precessing_IMBH.dat', dataset)
	plt.scatter(*dataset[:,:2].T)
	plt.show()
	quit()
#dataset = dataset[np.where(dataset[:,-1]>-10)]

if args.n_datapoints: dataset = dataset[:args.n_datapoints]
if args.verbose: print('Loaded dataset(s) with {} entries'.format(len(dataset)))

N_train = int(args.train_fraction*len(dataset))
#max_ll = np.max(dataset[:, -1])
#dataset[:, -1] = dataset[:, -1] - max_ll + 1
train_data, validation_data = dataset[:N_train,:D], dataset[N_train:,:D]
train_ll, validation_ll = dataset[:N_train, -1], dataset[N_train:, -1]

if args.verbose: print('Train {} | Validation {}'.format(len(train_data), len(validation_data)))

	######
	#	Training the flow and saving the output
	######

if args.load_flow:
	flow = STD_GW_Flow.load_flow(args.flow_file)
else:
	flow = STD_GW_Flow(D, n_layers = args.n_layers, hidden_features = args.hidden_features, has_constant = True)

	early_stopper_callback = early_stopper(patience = args.patience, min_delta = args.min_delta, temp_file = args.flow_file+'.checkpoint', verbose = args.verbose)
	optimizer = optim.Adam(flow.parameters(), lr=args.learning_rate)
	scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold = .02, factor = 0.5, patience = 4)
	
	history = flow.train_flow(args.loss_function, N_epochs = args.n_epochs,
		train_data = train_data, train_weights = train_ll,
		validation_data = validation_data, validation_weights = validation_ll,
		optimizer = optimizer, batch_size = args.batch_size, validation_step = 100,
		callback = early_stopper_callback, lr_scheduler = scheduler,
		boundaries = boundaries, verbose = args.verbose)

	if os.path.isfile(args.flow_file+'.checkpoint'): os.remove(args.flow_file+'.checkpoint')
	flow.save_weigths(args.flow_file)

if flow.constant:
	shift = flow.constant.detach().numpy()
else:
	with torch.no_grad():
		train_ll_flow = flow.log_prob(torch.Tensor(train_data[:10000])).numpy()
	shift = np.nanmedian(train_ll[:len(train_ll_flow)]-train_ll_flow)
if args.verbose: print('Log volume of the space (a.k.a. constant shift): ', shift)

	######
	#	Plotting
	######

if args.plot:
	with torch.no_grad():
		validation_ll_flow = flow.log_prob(torch.Tensor(validation_data[:10000])).numpy()
		plot_samples = flow.sample(10000).numpy()

	if not True: #DEBUG: see what happens to the tiling
		from mbank import tiling_handler
		t = tiling_handler('out_high_dimensional_bank_flow/tiling_high_dimensional_bank_flow.npy')
		t.flow = flow
		validation_ll_flow = 0.5*np.log(np.linalg.det(t.get_metric(validation_data[:10000], flow = True, kdtree = True)))
		shift = 0


	plt.figure()
	plt.hist((validation_ll_flow - validation_ll[:len(validation_ll_flow)]+shift)/np.log(10), histtype = 'step', bins = 100, label = 'flow', density = True)
	plt.legend()
	plt.xlabel(r"$\log_{10}(M_{flow}/M_{true})$")
	plt.savefig(args.run_dir+'flow_accuracy.png')

	plt.figure()
	m, M = int(np.min(validation_ll)/np.log(10)), int(np.max(validation_ll)/np.log(10))+1
	b_list = [i for i in range(m, M, 2)]
	b_list.insert(0, -np.inf)
	b_list.append(np.inf)
	log_M_diff = (validation_ll_flow - validation_ll[:len(validation_ll_flow)])+shift

	for start, stop in zip(b_list[:-1], b_list[1:]):
		ids_, = np.where(np.logical_and(
			validation_ll[:len(validation_ll_flow)]/np.log(10)>start,
			validation_ll[:len(validation_ll_flow)]/np.log(10)<stop)
		)
		print('[{}, {}]: {}'.format(start, stop, len(ids_)))
		if len(ids_)>10:
			plt.hist(log_M_diff[ids_]/np.log(10),
				histtype = 'step', bins = int(np.sqrt(len(ids_))), density = True,
				label = r"$\log_{10}M_{true} \in $"+'[{}, {}]'.format(start, stop))
	plt.xlabel(r"$\log_{10}(M_{pred}/M_{true})$")
	plt.legend(loc = 'upper left')
	plt.savefig(args.run_dir+'flow_accuracy_detailed.png')


	plot_tiles_templates(plot_samples, args.variable_format,
		tiling = None, dist_ellipse = None, save_folder = args.run_dir, show = False, title = 'Flow samples')
	plot_tiles_templates(validation_data[:10000], args.variable_format,
		tiling = None, dist_ellipse = None, save_folder = None, show = False, title = 'Validation data')
	if args.show: plt.show()
		
	
#### To access all the weights of the flow
#
#for transform in flow._transform._transforms:
#	for m in transform.modules():
#		for mm in m.parameters():
#			assert isinstance(mm, torch.nn.Parameter)

# For each mm, you can set requires_grad to False to disable training...

#[[isinstance(mm, torch.nn.Parameter) for mm in m.parameters()] for m in flow._transform._transforms[2].modules()]













