#!/usr/bin/env python
"""
mbank_train_flow
----------------

Executable to train a normalizing flow to reproduce the volume element of the probability distribution:

.. math
				
	p(\\theta) \propto \sqrt{|M(\\theta)|}

The script will create a dataset with random points drawn within the space an will train the flow with it.
The first part can be omitted by providing a home-made dataset.

To train a flow:

	mbank_train_flow --options-you-like

You can also load (some) options from an ini-file:

	mbank_train_flow --some-options other_options.ini

Make sure that the mbank is properly installed.

To know which options are available:

	mbank_train_flow --help
"""
import numpy as np
import matplotlib.pyplot as plt
import sys
import warnings
from ligo.lw.utils import load_filename

import torch
from torch import optim

from mbank import variable_handler, cbc_metric
from mbank.utils import load_PSD, avg_dist, plot_tiles_templates, get_boundaries_from_ranges
from mbank.parser import get_boundary_box_from_args, boundary_keeper
import mbank.parser

from mbank.flow import STD_GW_Flow, GW_Flow
from mbank.flow.utils import early_stopper

import emcee
from tqdm import tqdm
import argparse
import os
from lal import MTSUN_SI

############################################################################
def log_prob_initial_dataset(mceta, b_checker, sample_format):
	squeeze = (np.asarray(mceta).ndim == 1)
	mceta = np.atleast_2d(mceta)
	res = np.full(mceta[:,0].shape, -100000)
	ids_inside = b_checker(mceta, sample_format)
	if np.any(ids_inside):
		res[ids_inside] = -(np.log(2048/(5*np.pi)) + 13/3 * np.log(np.pi*args.f_min) + 10/3 * np.log(mceta[ids_inside,0]) + 8/5 * np.log(mceta[ids_inside,1]))
	if squeeze: res = res[0]
	return res
	

def get_random_masses(boundaries):

	assert boundaries.shape == (2,2)

	n_walkers = 100
	start_points = []
	out_variable_format = '{}_nonspinning'.format(var_handler.format_info[args.variable_format]['mass_format'])

	boundaries_checker = boundary_keeper(args)

	while len(start_points)<n_walkers:
		new_points = var_handler.convert_theta(np.random.uniform(*boundaries, (100, 2)), out_variable_format, sample_format)
		new_points = new_points[boundaries_checker(new_points, sample_format)]
		if len(new_points)>0:
			start_points.extend(new_points)
	start_points = np.array(start_points)[:n_walkers]
	
	sampler = emcee.EnsembleSampler(n_walkers, 2, log_prob_initial_dataset, args=[boundaries_checker, sample_format])
		#Burn-in
	points = sampler.run_mcmc(start_points, 400, tune=True, store=False, progress=args.verbose, progress_kwargs={'desc': 'Burn-in for mass samples'})
	
	new_masses = []
	for i in tqdm(range(args.n_datapoints//n_walkers+1), desc = 'Generating mass samples', disable = not args.verbose):
		points = sampler.run_mcmc(points, 10, tune=False, store=False, progress=False)
		new_masses.append(points.coords)

	new_masses = np.concatenate(new_masses, axis = 0)
	new_masses = var_handler.convert_theta(new_masses[:args.n_datapoints], sample_format, out_variable_format)

	return new_masses

##### Creating parser
parser = argparse.ArgumentParser(__doc__)

mbank.parser.add_general_options(parser)
mbank.parser.add_metric_options(parser)
mbank.parser.add_range_options(parser)
mbank.parser.add_flow_options(parser)

	#Options specific to this program
parser.add_argument(
	"--n-datapoints", default = None, type = int,
	help="Number of datapoints to be stored or loaded in the dataset")
parser.add_argument(
	"--dataset", default = ['dataset.dat'], type = str, nargs = '+',
	help="Files with the datasets (more than one are accpted). If the file does not exist, the dataset will be created.")
parser.add_argument(
	"--datapoints-file", default = None, type = str,
	help="Csv with a list of points to build the dataset: for each of them the likelihood will be computed and stored in the dataset")

parser.add_argument(
	"--ignore-boundaries", default = False, action = 'store_true',
	help="Whether to ignore the ranges given by command line. If set, the boudaries will be extracted from the dataset")

args, filenames = parser.parse_known_args()

	#updating from the ini file(s), if it's the case
for f in filenames:
	args = mbank.parser.updates_args_from_ini(f, args, parser)

####################################################################################################
	######
	#	Interpreting the parser and initializing variables
	######

if (args.psd is None) or (args.variable_format is None):
	raise ValueError("The arguments psd and variable_format must be set!")

var_handler = variable_handler()
assert args.variable_format in var_handler.valid_formats, "Wrong value {} for variable-format".format(args.variable_format)
D = var_handler.D(args.variable_format)

if args.run_dir is None: args.run_dir = './out_{}/'.format(args.run_name)
if not args.run_dir.endswith('/'): args.run_dir = args.run_dir+'/'
if not os.path.exists(args.run_dir): os.makedirs(args.run_dir)
if args.psd.find('/') <0: args.psd = args.run_dir+args.psd
if args.flow_file.find('/') <0: args.flow_file = args.run_dir+args.flow_file

for i, f in enumerate(args.dataset):
	if f.find('/') <0: f = args.run_dir+f
	args.dataset[i] = f

generate_dataset = not np.any([os.path.isfile(f) for f in args.dataset])

plot_folder = args.run_dir if args.plot else None
if args.verbose: print("## Running: ", args.run_name)

boundaries = None if args.ignore_boundaries else get_boundary_box_from_args(args)

#TODO: should generate dataset be a standalone script mbank_generate_dataset?
if generate_dataset:

	sample_format = 'mceta_nonspinning'
	assert args.n_datapoints, "--n-datapoints must be set if the dataset is generated"

	######
	#	Loading PSD and initializing metric
	######
	m = cbc_metric(args.variable_format,
		PSD = load_PSD(args.psd, args.asd, args.ifo, df = args.df),
		approx = args.approximant,
		f_min = args.f_min, f_max = args.f_max)

	######
	#	Drawing initial points (either from file or from sampling)
	#	Mass boundaries are complicated, while the rest is just a rectangle in the coordinates
	######

	if args.datapoints_file:
	
		datapoints = np.loadtxt(args.datapoints_file)[:args.n_datapoints]

	else:
	
		#Random sampling
			#setting spin & angles boundaries
		assert boundaries is not None, "If dataset has to be generated, option --ignore-boundaries must be removed"
		datapoints = np.random.uniform(*boundaries, (args.n_datapoints, D))
		
			#Checking boundaries
		if False:
			plot_tiles_templates(datapoints[boundary_keeper(args)(datapoints, args.variable_format)], args.variable_format, show = True)
			quit()
		
		new_masses = get_random_masses(boundaries[:,:2])
		datapoints[:,:2] = new_masses
		
	metric_list = []
	for d in tqdm(datapoints, disable = not args.verbose, desc = 'Generating dataset'):
		try:
			metric = m.get_metric(d, metric_type = args.metric_type)
			metric_list.append(np.array([*metric.flatten(), 0.5*np.log(np.abs(np.linalg.det(metric)))]))
		except KeyboardInterrupt:
			warnings.warn("KeyboardInterrupt: interuppting dataset generation and continuing with the flow training")
			break
	
	to_save = np.concatenate([datapoints[:len(metric_list)], metric_list], axis = 1)
	np.savetxt(args.dataset[0], to_save,
		header = 'variable format: {0}\ntheta ({1},) | metric ({1}*{1},) | log_pdf (1,) '.format(args.variable_format, D))

	if args.verbose: print("Dataset saved")
	del datapoints, to_save

	######
	#	Loading the dataset(s) and performing train and test splitting
	######

dataset = []
for f in args.dataset:
	if not os.path.isfile(f): continue
	
	if args.verbose: print("Loading dataset {}".format(f))
	
	new_dataset = np.loadtxt(f)
	assert new_dataset.ndim == 2, "Wrong dimensionality of the dataset, must be 2 dimensional"
	assert new_dataset.shape[1] == D**2+D+1, "Wrong format of the datset! Each row must be {} dimensional, i.e. with format D | D**2 | 1".format(D*(D+1)+1)
	
		#removing possible nans
	new_dataset = new_dataset[np.where(~np.any(np.isnan(new_dataset), axis = 1))]
	
	dataset.append(new_dataset)

dataset = np.concatenate(dataset, axis = 0)

if not args.ignore_boundaries:
	boundaries_checker = boundary_keeper(args)
	ids_ = boundaries_checker(dataset[:,:D], args.variable_format)
	dataset = dataset[ids_]

if not True:
	#dataset = dataset[np.where(dataset[:,-1]>-12)]
	dataset = dataset[np.where(dataset[:,-1]<=-12)]
	np.random.shuffle(dataset)
	np.savetxt('out_O3_bank_flow/dataset_paper_precessing_IMBH.dat', dataset)
	plt.scatter(*dataset[:,:2].T)
	plt.show()
	quit()
#dataset = dataset[np.where(dataset[:,-1]>-10)]

if args.n_datapoints: dataset = dataset[:args.n_datapoints]
if args.verbose: print('Loaded dataset(s) with {} entries'.format(len(dataset)))

N_train = int(args.train_fraction*len(dataset))
#max_ll = np.max(dataset[:, -1])
#dataset[:, -1] = dataset[:, -1] - max_ll + 1
train_data, validation_data = dataset[:N_train,:D], dataset[N_train:,:D]
train_ll, validation_ll = dataset[:N_train, -1], dataset[N_train:, -1]

if args.verbose: print('Train {} | Validation {}'.format(len(train_data), len(validation_data)))

	######
	#	Training the flow and saving the output
	######

if args.load_flow:
	flow = STD_GW_Flow.load_flow(args.flow_file)
else:
	flow = STD_GW_Flow(D, n_layers = args.n_layers, hidden_features = args.hidden_features, has_constant = True)

	early_stopper_callback = early_stopper(patience = args.patience, min_delta = args.min_delta, temp_file = args.flow_file+'.checkpoint', verbose = args.verbose)
	optimizer = optim.Adam(flow.parameters(), lr=args.learning_rate)
	scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold = .02, factor = 0.5, patience = 4)
	
	history = flow.train_flow(args.loss_function, N_epochs = args.n_epochs,
		train_data = train_data, train_weights = train_ll,
		validation_data = validation_data, validation_weights = validation_ll,
		optimizer = optimizer, batch_size = args.batch_size, validation_step = 100,
		callback = early_stopper_callback, lr_scheduler = scheduler,
		boundaries = boundaries, verbose = args.verbose)

	if os.path.isfile(args.flow_file+'.checkpoint'): os.remove(args.flow_file+'.checkpoint')
	flow.save_weigths(args.flow_file)

if flow.constant:
	shift = flow.constant.detach().numpy()
else:
	with torch.no_grad():
		train_ll_flow = flow.log_prob(torch.Tensor(train_data[:10000])).numpy()
	shift = np.nanmedian(train_ll[:len(train_ll_flow)]-train_ll_flow)
if args.verbose: print('Log volume of the space (a.k.a. constant shift): ', shift)

	######
	#	Plotting
	######

if args.plot:
	with torch.no_grad():
		validation_ll_flow = flow.log_prob(torch.Tensor(validation_data[:10000])).numpy()
		plot_samples = flow.sample(10000).numpy()

	if not True: #DEBUG: see what happens to the tiling
		from mbank import tiling_handler
		t = tiling_handler('out_high_dimensional_bank_flow/tiling_high_dimensional_bank_flow.npy')
		t.flow = flow
		validation_ll_flow = 0.5*np.log(np.linalg.det(t.get_metric(validation_data[:10000], flow = True, kdtree = True)))
		shift = 0


	plt.figure()
	plt.hist((validation_ll_flow - validation_ll[:len(validation_ll_flow)]+shift)/np.log(10), histtype = 'step', bins = 100, label = 'flow', density = True)
	plt.legend()
	plt.xlabel(r"$\log_{10}(M_{flow}/M_{true})$")
	plt.savefig(args.run_dir+'flow_accuracy.png')

	plt.figure()
	m, M = int(np.min(validation_ll)/np.log(10)), int(np.max(validation_ll)/np.log(10))+1
	b_list = [i for i in range(m, M, 2)]
	b_list.insert(0, -np.inf)
	b_list.append(np.inf)
	log_M_diff = (validation_ll_flow - validation_ll[:len(validation_ll_flow)])+shift

	for start, stop in zip(b_list[:-1], b_list[1:]):
		ids_, = np.where(np.logical_and(
			validation_ll[:len(validation_ll_flow)]/np.log(10)>start,
			validation_ll[:len(validation_ll_flow)]/np.log(10)<stop)
		)
		print('[{}, {}]: {}'.format(start, stop, len(ids_)))
		if len(ids_)>10:
			plt.hist(log_M_diff[ids_]/np.log(10),
				histtype = 'step', bins = int(np.sqrt(len(ids_))), density = True,
				label = r"$\log_{10}M_{true} \in $"+'[{}, {}]'.format(start, stop))
	plt.xlabel(r"$\log_{10}(M_{pred}/M_{true})$")
	plt.legend(loc = 'upper left')
	plt.savefig(args.run_dir+'flow_accuracy_detailed.png')


	plot_tiles_templates(plot_samples, args.variable_format,
		tiling = None, dist_ellipse = None, save_folder = args.run_dir, show = False, title = 'Flow samples')
	plot_tiles_templates(validation_data[:10000], args.variable_format,
		tiling = None, dist_ellipse = None, save_folder = None, show = False, title = 'Validation data')
	if args.show: plt.show()
		
	
#### To access all the weights of the flow
#
#for transform in flow._transform._transforms:
#	for m in transform.modules():
#		for mm in m.parameters():
#			assert isinstance(mm, torch.nn.Parameter)

# For each mm, you can set requires_grad to False to disable training...

#[[isinstance(mm, torch.nn.Parameter) for mm in m.parameters()] for m in flow._transform._transforms[2].modules()]













